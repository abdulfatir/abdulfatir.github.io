<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Abdul Fatir  Ansari


  | Introduction to Gradient Flows in the 2-Wasserstein Space

</title>
<meta name="description" content="Abdul Fatir Ansari, Machine Learning Scientist
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2020/Gradient-Flows/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-MF3HZJPRL1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-MF3HZJPRL1');
</script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams',
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      processEscapes: true,
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

  <!-- Nav Bar -->
  <nav id="navbar"
    class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://abdulfatir.com//">
        <span class="font-weight-bold">Abdul Fatir</span> 
        Ansari
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
            <a class="nav-link" href="/news/">
              news
              
            </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">
              publications
              
            </a>
          </li>
          
          
          
          <!-- CV -->
          <li class="nav-item">
            <a class="nav-link" href="https://drive.google.com/open?id=1T9tMY1NQQTTFE2sIYXjuVDUIufE5xXB6"
              target="_blank">curriculum vitae</a>
          </li>
          
        </ul>
      </div>
    </div>
  </nav>

</header>

    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Introduction to Gradient Flows in the 2-Wasserstein Space</h1>
    <p class="post-meta">December 24, 2020</p>
  </header>

  <article class="post-content">
    <hr />
<p><span style="font-size: 80%; color: gray;">
<strong>Note</strong>: This article is a gentle introduction to the concept of gradient flows in the Wasserstein space. The target audience are researchers working in the area of machine learning, not mathematicians (Forgive my non-rigorous soul, father!). The “flow” of arguments in this article follows the excellent <a href="https://arxiv.org/abs/1609.03890">overview of gradient flows</a> by <a href="http://math.univ-lyon1.fr/~santambrogio/">Filippo Santambrogio</a> and <a href="https://www.youtube.com/watch?v=zzGBxAqJV0Q">this lecture</a> by <a href="https://web.njit.edu/~bdfroese/">Brittany Hamfeldt</a>.
</span></p>

<hr />

\[\definecolor{purple}{RGB}{114,0,172}
\definecolor{maroon}{RGB}{133, 20, 75}
\definecolor{blue}{RGB}{18,110,213}\]

<p>A gradient flow is a curve following the direction of steepest descent of a function(-al). For example, let $E: \mathbb{R}^n \to \mathbb{R}$ be a smooth, convex energy function. The gradient flow of $E$ is the solution to the following initial value problem,</p>

\[\begin{align}
x'(t) &amp;= -\nabla E(x(t))\tag{1}\label{eq:euclidean-gf},\\
x(0) &amp;= x_0.
\end{align}\]

<p>We seek to extend the idea of steepest descent curves to metric spaces, specifically the 2-Wasserstein space (defined later); however, tangent vectors and gradients have no definitions in metric spaces. In the following, we will characterize the gradient flow in Eq. ($\ref{eq:euclidean-gf}$) as the limit curve of a discrete-time scheme. We begin by discretizing the Ordinary Differential Equation (ODE) using the implicit Euler method,</p>

\[\begin{align}
\frac{x_{n+1} - x_{n}}{\tau} &amp;= -\nabla E(x_{n+1})\tag{2}\label{eq:back-euler},
\end{align}\]

<p>which can equivalently be written as</p>

\[\begin{align}
\nabla\left(\frac{|x - x_{n}|^2}{2\tau} + E(x)\right)\Bigg|_{x=x_{n+1}} = 0\tag{3}\label{eq:back-euler-optim}.
\end{align}\]

<p>Eq. ($\ref{eq:back-euler-optim}$) looks like an optimality condition and consequently $x_{n+1}$ can be written as the solution to a minimization problem,</p>

\[\begin{align}
x_{n+1} = \mathrm{arg}\min\left(\frac{|x - x_{n}|^2}{2\tau} + E(x)\right)\tag{4}\label{eq:mms-gf}.
\end{align}\]

<p>We have converted the discrete scheme into a form that does not involve gradients. This discrete scheme can be generalized to a metric space $(\mathcal{X}, d)$ where the space $\mathcal{X}$ is endowed with the metric $d$. Let $F: \mathcal{X} \to \mathbb{R}$ be a functional that is lower semi continuous and bounded below<span style="font-size: 70%; color: gray;"><sup>1</sup></span>. The equivalent discrete scheme in this metric space is given by,</p>

\[\begin{align}
x_{n+1}^\tau \in \mathrm{arg}\min\left(\frac{d(x, x_{n}^\tau)^2}{2\tau} + F(x)\right)\tag{5}\label{eq:gmm-gf},
\end{align}\]

<p>where we have replaced the Euclidean metric with the metric $d$. We can define a piecewise constant, continuous-time interpolation from the discrete scheme in Eq. ($\ref{eq:gmm-gf}$) as follows</p>

\[x^{\tau}(t) = x_n^\tau\qquad \text{if}\:\: t \in ((n-1)\tau, n\tau]\tag{6}\label{eq:pc-interpolation}.\]

<p>Gradient flows can now be characterized by studying the discrete scheme in the limit $\tau \to 0$. This discrete scheme is called the minimizing movement scheme and a curve $x: [0, T] \to \mathcal{X}$ is called Generalized Minimizing Movements (GMM) if there exists a sequence of time steps $\tau_j \to 0$ such that the sequence of piecewise constant curves in Eq. (\ref{eq:pc-interpolation}) converges uniformly to $x$.</p>

<p>Before discussing the 2-Wasserstein space, we will define some quantities in metric spaces that will be referred to later.</p>

<p><strong>Metric Derivative</strong> For a curve $x: [0, T] \to \mathcal{X}$ valued in a metric space $\mathcal{X}$, we can define the modulus of $x’(t)$ (i.e., the speed of the curve instead of its velocity as one would do in a vector space) as</p>

\[|x'|(t) \triangleq \lim_{h \to 0} \frac{d(x(t), x(t + h))}{|h|}.\]

<p><strong>AC Curves in a Metric Space</strong> A curve $x: [0, T] \to \mathcal{X}$ is said to be absolutely continuous (AC) if there exists a $g \in L^1([0, 1])$ such that $d(x(t_0), x(t_1)) \leq \int_{t_0}^{t_1}g(s)ds$ for every $t_0 &lt; t_1$.</p>

<h2 id="background-in-optimal-transport">Background in Optimal Transport</h2>

<p>Before defining the Wasserstein space of probability measures we introduce some ideas from optimal transport that will be used later. The optimal transport (OT) problem seeks to transport mass from a source measure $\mu$ to a target measure $\nu$ with respect to a given cost function $c$ in an “optimal” fashion. There are two popular formulations of the optimal transport problem due to Monge and Kantorovich.</p>

<p><strong>Monge’s Formulation</strong> Monge’s formulation seeks to find an (optimal) transport map $T$ that pushes forward $\mu$ to $\nu$ (often denoted as $T_{\sharp}\mu = \nu$)<span style="font-size: 70%; color: gray;"><sup>2</sup></span> while minimizing the following quantity</p>

\[\inf_{T_{\sharp}\mu = \nu} \int_{\mathcal{X}} c(T(x), x)d\mu(x)\tag{MP}.\]

<p>Note that we’re looking for a transport map and are not allowed to “break up mass”. Consider the problem of transporting a dirac measure at $0$ to a measure that assigns equal mass to $-1$ and $+1$. It is not possible to construct a Monge map for such a transport problem.</p>

<p><strong>Kantorovich’s Formulation</strong> Kanotrovich’s formulation seeks an (optimal) transport plan $\gamma$ that transports mass from $\mu$ to $\nu$ with respect to the cost function $c$ such that the marginals of $\gamma$ are $\mu$ and $\nu$.</p>

\[\inf_{\gamma \in \Gamma(\mu, \nu)} \int_{\mathcal{X} \times \mathcal{Y}} c(x, y)d\gamma(x,y)\tag{KP}\label{eq:ot-kp}\]

<p>where $\Gamma(\mu, \nu)$ is the set of transport plans with the correct marginals, i.e.,</p>

\[\Gamma(\mu, \nu) = \{\gamma | (\pi_{\mathcal{X}})_\sharp\gamma = \mu, (\pi_{\mathcal{Y}})_\sharp\gamma=\nu\}\]

<p>where \((\pi_{\mathcal{X}})_\sharp\gamma\) denotes the projection of $\gamma$ onto \(\mathcal{X}\). Monge’s formulation is a special case of Kanotrovich’s formulation such that whenever the optimal Monge map \(T^*\) exists, the corresponding Kantorovich transport plan is given by \(\gamma^* = (\mathrm{id} \times T^*)_\sharp\mu\) where \(\mathrm{id}\) is the identity map.</p>

<p><strong>Kantorovich duality</strong> Kanotrovich’s formulation has an equivalent dual form given by</p>

\[\sup_{\substack{f,g\\f(x)+g(y) \leq c(x, y)}} \int_\mathcal{X} fd\mu(x) + \int_\mathcal{Y} gd\nu(y)\tag{KPDual}\label{eq:ot-kpdual},\]

<p>where the supremum runs over the set of bounded and continuous functions $f: \mathcal{X} \to \mathbb{R}$ and $g: \mathcal{Y} \to \mathbb{R}$ such that \(f(x)+g(y) \leq c(x, y)\). Eq. (\ref{eq:ot-kpdual}) can be written as a supremum over a single function $\psi$ with its corresponding $c$-transform<span style="font-size: 70%; color: gray;"><sup>3</sup></span> $\psi^c$,</p>

\[\sup_{\psi \in \Psi^c} \int_\mathcal{X} \psi d\mu(x) + \int_\mathcal{Y} \psi^c d\nu(y)\tag{7}\label{eq:ot-kpdual-ctransform},\]

<p>where \(\psi^c(y) = \inf_{x} \left\{c(x, y) - \psi(x)\right\}\) and $\Psi^c$ is the set of $c$-concave functions where a $c$-concave function is a function $\psi$ for which there exists a function $\varphi$ such that $\psi = \varphi^c$. The function(s) $\psi$ realizing the maximum in Eq. (\ref{eq:ot-kpdual-ctransform}) are called Kantorovich potentials.</p>

<p><strong>OT with Quadratic Cost</strong> For the special case of optimal transport with the quadratic cost \(c(x, y) = \frac{\|x - y\|^2}{2}\), there exists a unique optimal transport plan of the form $(\mathrm{id} \times T^*)_\sharp\mu$ provided that $\mu$ is absolutely continuous. Further, there exists at least one Kantorovich potential $\psi$ such that its gradient $\nabla \psi$ is uniquely determined. The form of the transport plan implies the existence of an optimal Monge map \(T^*\) which is related to the potential $\psi$ by \(T^*(x) = x - \nabla\psi(x)\).</p>

<h2 id="the-2-wasserstein-space">The 2-Wasserstein Space</h2>

<p>Let $\mathcal{P}_2(\Omega)$ be the set of probability measures on a domain $\Omega \subset \mathbb{R}^d$ with finite second moments, i.e.,</p>

\[\mathcal{P}_2(\Omega) \triangleq \left\{\mu \Big| \int_{\Omega}|x|^2d\mu(x) &lt; \infty\right\}.\]

<p>The 2-Wasserstein distance between probability measures $\mu \in \mathcal{P}_2(\Omega)$ and $\nu \in \mathcal{P}_2(\Omega)$ is defined as,</p>

\[{\color{blue}\mathcal{W}_2(\mu, \nu) \triangleq \left(\inf_{\gamma \in \Gamma(\mu, \nu)} \int \|x-y\|^2d\gamma(x,y)\right)^{1/2}},
\tag{W2}\label{eq:w2primal}\]

<p>where $\Gamma(\mu, \nu)$ is a set of all possible couplings with marginals $\mu$ and $\nu$. We can see that $\mathcal{W}_2$ is equal to the square root of Eq. (\ref{eq:ot-kp}) with \(c(x,y) = \|x-y\|^2\). It can be shown that $\mathcal{W}_2$ satisfies the axioms of a metric on $\mathcal{P}_2(\Omega)$ and convergence with respect to $\mathcal{W}_2$ is equivalent to weak convergence of probability measures, i.e., for a sequence of measures \((\mu_k)_{k \in \mathbb{N}}\) in $\mathcal{P}_2(\Omega)$, \(\mu_k \to \mu \Longleftrightarrow \mathcal{W}_2(\mu_k, \mu) \to 0\). The 2-Wasserstein space $(\mathcal{P}_2(\Omega), \mathcal{W}_2)$ is the metric space of probability measures $\mathcal{P}_2(\Omega)$ endowed with the 2-Wasserstein ($\mathcal{W}_2$) metric.</p>

<p><strong>AC Curves in the 2-Wasserstein Space</strong> Let \(\{\mu_t\}_{t\in[0,1]}\) be an absolutely continuous curve in $(\mathcal{P}_2(\Omega), \mathcal{W}_2)$. Then, for $t \in [0, 1]$ there exists a velocity field $\mathbf{v}_t \in L^2(\mu_t; \mathbb{R}^d)$ such that the continuity equation</p>

\[{\color{blue}\frac{\partial \mu_t}{\partial t} + \nabla\cdot(\mathbf{v}_t\mu_t) = 0}\tag{CE}\label{eq:continuity-eq}\]

<p>is satisfied and</p>

\[\|\mathbf{v}_t\|_{L^2(\mu_t)} = |\mu'|(t).\tag{8}\label{eq:velocity-norm-metric-derivative}\]

<p>The proof of these statements is beyond the scope of this article (frankly, I don’t know how to prove it). However, for the second statement, we can consider two measures $\mu_t$ and $\mu_{t+h}$. There are several ways to transport mass from $\mu_t$ to $\mu_{t+h}$ one of which is optimal in the OT sense. Let $T^*$ be the OT map between $\mu_t$ and $\mu_{t+h}$. We can define \(\mathbf{v}_t(x)\) as the discrete velocity of the particle $x$ at time $t$ given by \(\mathbf{v}_t(x) = (T^*(x)-x)/h\) (i.e., displacement/time). We can intuitively see that in the limit \(h \to 0\), Eq. (\ref{eq:velocity-norm-metric-derivative}) holds, since</p>

\[\begin{align}
\|\mathbf{v}_t\|_{L^2(\mu_t)} &amp;= \left(\int_{\mathbb{R}^d}\left|\frac{(T^*(x)-x)}{h}\right|^2d\mu_t(x)\right)^{1/2},\\
&amp;= \frac{1}{h}\mathcal{W}_2(\mu_{t},\mu_{t+h}).
\end{align}\]

<h2 id="gradient-flows-in-the-2-wasserstein-space">Gradient Flows in the 2-Wasserstein Space</h2>

<p>Now that we have established that absolutely continuous curves in the 2-Wasserstein space satisfy the continuity equation, our task is to link Partial Differential Equations (PDEs) of the form of Eq. (\ref{eq:continuity-eq}) to the discrete-time minimizing movement scheme,</p>

\[\begin{align}
\rho_{n+1}^\tau &amp;\in \mathrm{arg}\min\left(\frac{\mathcal{W}_2(\rho, \rho_{n}^\tau)^2}{2\tau} + \mathcal{F}(\rho)\right)\tag{9}\label{eq:mms-w2},\\
\rho^\tau(t) &amp;= \rho^\tau_n\qquad \text{if}\:\: t \in ((n-1)\tau, n\tau],
\end{align}\]

<p>where $\mathcal{F}: \mathcal{P}_2(\Omega) \to \mathbb{R}$ is a functional on the 2-Wasserstein space that is lower semi-continuous and bounded below. Note that we now denote probability measures using $\rho$ to indicate the fact that they are absolutely continuous measures with smooth densities. Concretely, now our task is to find the velocity field $\mathbf{v}_t$ such that the solution to the continuity equation agrees with \(\lim_{\tau\to 0}\rho^\tau(t)\).</p>

<p>Let us now investigate the optimality condition of Eq. (\ref{eq:mms-w2}). By analogy to the optimality condition for functions where we set the first derivative of the function equal to 0, we can set the first variation of a functional defined on the 2-Wasserstein space equal to a constant (Why?<span style="font-size: 70%; color: gray;"><sup>2</sup></span>). The first variation \({\color{purple}\frac{\delta\mathcal{G}}{\delta\rho}}\) of a functional \(\mathcal{G}: \mathcal{P}_2(\Omega) \to \mathbb{R}\) at a point \(\rho\), if it exists, is defined (up to additive constants) as</p>

\[\frac{d}{d\epsilon} \mathcal{G}(\rho + \epsilon\chi)\Bigg|_{\epsilon = 0} = \int {\color{purple}\frac{\delta\mathcal{G}}{\delta\rho}(\rho)}d\chi(x).\]

<p>Note that $\chi$ is chosen such that $\rho + \epsilon\chi \in \mathcal{P}_2(\Omega)$; this can be done by setting $\chi = \sigma - \rho$ for some $\sigma \in \mathcal{P}_2(\Omega)$.</p>

<p>We now compute the first variation of the functional on the r.h.s. of Eq. (\ref{eq:mms-w2}) and set it equal to a constant,</p>

\[\begin{align}
\frac{\delta}{\delta \rho}\left[\frac{\mathcal{W}_2(\rho, \rho_{n}^\tau)^2}{2\tau} + \mathcal{F}(\rho)\right]\Bigg|_{\rho=\rho^\tau_{n+1}} &amp;= \mathrm{constant},\\
\left[\frac{1}{2\tau}\frac{\delta\mathcal{W}_2(\rho, \rho_{n}^\tau)^2}{\delta \rho} + \frac{\delta\mathcal{F}}{\delta \rho}(\rho)\right]\Bigg|_{\rho=\rho^\tau_{n+1}} &amp;= \mathrm{constant}.\tag{10}\label{eq:optimality-eq}\\
\end{align}\]

<p>The first variation of $\mathcal{F}$ depends on its specific form; therefore, we begin by deriving an expression for the first variation of the squared 2-Wasserstein distance.</p>

<p><strong>First Variation of the Squared 2-Wasserstein Distance</strong> We begin by writing the expression of the squared Wasserstein distance in its primal form,</p>

\[\mathcal{W}^2_2(\mu, \nu) = 2\color{maroon}{\inf_{\gamma \in \Gamma(\mu, \nu)} \int \frac{\|x-y\|^2}{2}d\gamma(x,y)}.\]

<p>The expression in <span style="color:#85144b">maroon</span> is the Kantorovich formulation of OT with the quadratic cost \(c(x,y) = \frac{\|x-y\|^2}{2}\). We have multiplied and divided the expression by 2 to utilize the properties of OT with the quadratic cost. Let us convert the expression into its dual form using $c$-concave functions,</p>

\[\mathcal{W}^2_2(\mu, \nu) = 2{\sup_{\psi \in \Psi^c} \int \psi d\mu(x) + \int \psi^c d\nu(y)}.\]

<p>Let $\psi_*$ be a Kantorovich potential that achieves the supremum in the above equation. We can now replace \(\psi\) with \(\psi_*\) and remove the \(\sup\) operator,</p>

\[\mathcal{W}^2_2(\mu, \nu) = 2 \int \psi_* d\mu(x) + \int \psi_*^c d\nu(y).\]

<p>Perturbing $\mu$ along $\chi$ and differentiating the resulting expression with respect to $\epsilon$ at $\epsilon=0$ we get,</p>

\[\frac{d}{d\epsilon} \mathcal{W}^2_2(\mu + \epsilon\chi, \nu)\Bigg|_{\epsilon = 0} = \int {\color{purple}\underset{\frac{\delta\mathcal{W}^2_2(\mu, \nu)}{\delta\mu}}{\underbrace{2\psi_*}}} d\chi(x).\]

<p><strong>Deriving an Expression for Particle Velocity</strong> The above equation shows that the first variation of the squared Wasserstein distance between measures $\mu$ and $\nu$ is equal to \(2\psi_*\) where \(\psi_*\) is the Kantorovich potential associated with optimal transport from $\mu$ to $\nu$ with the quadratic cost. We can substitute this result into Eq. (\ref{eq:optimality-eq}) and get,</p>

\[\left[\frac{\varphi_*}{\tau} + \frac{\delta\mathcal{F}}{\delta \rho}(\rho^\tau_{n+1})\right] = \mathrm{constant}\tag{11}\label{eq:optimality-cond-2},\]

<p>where $\varphi_*$ is the Kantorovich potential associated with optimal transport from $\rho^\tau_{n+1}$ to $\rho^\tau_{n}$. As mentioned earlier, the Kantorovich potential and the Monge transport map for the quadratic cost are related by \(T^*(x) = x - \nabla\varphi_*(x)\). Taking the gradient with respect to $x$ on both sides of Eq. (\ref{eq:optimality-cond-2}) and substituting \(\nabla\varphi_*(x) = -(T^*(x)-x)\) we get</p>

\[\frac{(T^*(x)-x)}{\tau} = \nabla\frac{\delta\mathcal{F}}{\delta \rho}(\rho^\tau_{n+1})(x).\]

<p>Note that the expression \(\frac{(T^*(x)-x)}{\tau}\) can be thought of as the discrete velocity of a particle moving backwards in time from $\rho^\tau_{n+1}$ to $\rho^\tau_{n}$ in an optimal transport sense. We can intuitively see how this expression would become the instantaneous velocity in the limit $\tau \to 0$. This gives us an expression for the instantaneous forward velocity of a particle,</p>

\[{\color{blue}\mathbf{v}_t(x) = -\nabla\frac{\delta\mathcal{F}}{\delta \rho}(\rho_t)(x)}.\]

<p>Plugging this expression into the continuity equation we get the expression of the gradient flow of a functional $\mathcal{F}$ in the Wasserstein space</p>

\[{\color{blue}\frac{\partial \rho_t}{\partial t} - \nabla\cdot\left(\nabla\frac{\delta\mathcal{F}}{\delta \rho}(\rho_t)\rho_t\right) = 0}\]

<h3 id="some-famous-examples-of-gradient-flows-in-mathcalp_2omega-mathcalw_2">Some famous examples of gradient flows in $(\mathcal{P}_2(\Omega), \mathcal{W}_2)$</h3>

<p><strong>Negative Differential Entropy</strong> When the functional $\mathcal{F}$ is defined to be the negative differential entropy, i.e.,</p>

\[\mathcal{F} \triangleq \int \rho(x)\log\rho(x)dx,\]

<p>the velocity is given by \(\mathbf{v}_t = -\frac{1}{\rho(x)}\nabla\rho(x)\) and we recover the famous heat equation as the gradient flow of negative entropy in the 2-Wasserstein space,</p>

\[{\color{blue}\frac{\partial \rho_t}{\partial t} - \Delta\rho = 0}.\tag{HE}\]

<p><strong>$f$-Divergence</strong> When $\mathcal{F}$ is defined to be the $f$-divergence between $\rho_t$ and a fixed target density $\mu$, i.e.,</p>

\[\mathcal{F} \triangleq \int f\left(\frac{\rho(x)}{\mu(x)}\right)\mu(x)dx,\]

<p>where $f$ is a twice-differentiable convex function with $f(1) = 0$, the gradient flow is given by the following PDE,</p>

\[\frac{\partial \rho_t}{\partial t} - \nabla\cdot\left(\rho(x)\nabla f'\left(\frac{\rho(x)}{\mu(x)}\right)\right) = 0.\]

<p>For the special case of the KL divergence, i.e., $f = r\log r$, we recover the Fokker-Plank Equation,</p>

\[{\color{blue}\frac{\partial \rho_t}{\partial t} + \nabla\cdot(\rho(x)\nabla\log\mu(x)) - \Delta\rho(x) = 0}.\tag{FPE}\]

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>Gradient flows have been a popular tool in the analysis of PDEs. Recently, gradient flows of various distances/divergences used in machine learning have been proposed [3, 4, 5, 6, 7] and have been used for generative modeling [4, 5, 7] and sample refinement in generative models [8], among other cool applications [5]. This article is my attempt at providing some background for readers unfamiliar with gradient flows.</p>

<h2 id="references">References</h2>

<div style="font-size: 80%">

  <p>[1] Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview. Bulletin of Mathematical Sciences, 7(1), 2017.</p>

  <p>[2] Brittany Hamfeldt. Optimal Transport - Gradient Flows in the Wasserstein Metric. <a href="https://www.youtube.com/watch?v=zzGBxAqJV0Q">Video Lecture</a>, 2019.</p>

  <p>[3] Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum mean discrepancy gradient flow. In NeurIPS, 2019.</p>

  <p>[4] Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert Stoter. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In ICML, 2019.</p>

  <p>[5] Youssef Mroueh, Tom Sercu, and Anant Raj. Sobolev descent. In AISTATS, 2019.</p>

  <p>[6] Qiang Liu. Stein variational gradient descent as gradient flow. In NeurIPS, 2017.</p>

  <p>[7] Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang, Can Yang, and Shunkang Zhang. Deep generative learning via variational gradient flow. In ICML, 2019.</p>

  <p>[8] Abdul Fatir Ansari, Ming Liang Ang, and Harold Soh. Refining Deep Generative Models via Wasserstein Gradient Flows. arXiv preprint arXiv:2012.00780, 2020.</p>
</div>

<hr />
<h3 id="footnotes">Footnotes</h3>
<p><span style="font-size: 70%; color: gray;"><sup>1</sup> Note that we no longer require $F$ to be convex but other regularity conditions (lower semi-continuity and boundedness from below) are required for the existence of minimizers.</span></p>

<p><span style="font-size: 70%; color: gray;"><sup>2</sup> $T_{\sharp}\mu = \nu$ means that if we apply the map $T$ to samples from $\mu$ we get samples that are distributed according to $\nu$.</span></p>

<p><span style="font-size: 70%; color: gray;"><sup>3</sup> Please check <a href="http://abdulfatir.com/Wasserstein-Distance/">this article</a> for an intuitive introduction to the $c$-transform of a function. </span></p>

<p><span style="font-size: 70%; color: gray;"><sup>4</sup> The first variation is defined up to additive constants since $\chi$ is a zero-mean measure (it’s a difference of two probability measures).</span></p>

  </article>

  
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'abdulfatir';
      var disqus_identifier = '/blog/2020/Gradient-Flows';
      var disqus_title      = "Introduction to Gradient Flows in the 2-Wasserstein Space";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2023 Abdul Fatir  Ansari.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
