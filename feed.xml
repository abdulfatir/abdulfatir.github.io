<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://abdulfatir.com//feed.xml" rel="self" type="application/atom+xml"/><link href="https://abdulfatir.com//" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-07T11:24:31+00:00</updated><id>https://abdulfatir.com//feed.xml</id><title type="html">blank</title><subtitle>Abdul Fatir Ansari, Machine Learning Scientist </subtitle><entry><title type="html">Introduction to Gradient Flows in the 2-Wasserstein Space</title><link href="https://abdulfatir.com//blog/2020/Gradient-Flows/" rel="alternate" type="text/html" title="Introduction to Gradient Flows in the 2-Wasserstein Space"/><published>2020-12-24T21:01:00+00:00</published><updated>2020-12-24T21:01:00+00:00</updated><id>https://abdulfatir.com//blog/2020/Gradient-Flows</id><content type="html" xml:base="https://abdulfatir.com//blog/2020/Gradient-Flows/"><![CDATA[<hr/> <p><span style="font-size: 80%; color: gray;"> <strong>Note</strong>: This article is a gentle introduction to the concept of gradient flows in the Wasserstein space. The target audience are researchers working in the area of machine learning, not mathematicians (forgive my non-rigorous soul). The “flow” of arguments in this article follows the excellent <a href="https://arxiv.org/abs/1609.03890">overview of gradient flows</a> by <a href="http://math.univ-lyon1.fr/~santambrogio/">Filippo Santambrogio</a> and <a href="https://www.youtube.com/watch?v=zzGBxAqJV0Q">this lecture</a> by <a href="https://web.njit.edu/~bdfroese/">Brittany Hamfeldt</a>. </span></p> <hr/> \[\definecolor{purple}{RGB}{114,0,172} \definecolor{maroon}{RGB}{133, 20, 75} \definecolor{blue}{RGB}{18,110,213}\] <p>A gradient flow is a curve following the direction of steepest descent of a function(-al). For example, let $E: \mathbb{R}^n \to \mathbb{R}$ be a smooth, convex energy function. The gradient flow of $E$ is the solution to the following initial value problem,</p> \[\begin{align} x'(t) &amp;= -\nabla E(x(t))\tag{1}\label{eq:euclidean-gf},\\ x(0) &amp;= x_0. \end{align}\] <p>We seek to extend the idea of steepest descent curves to metric spaces, specifically the 2-Wasserstein space (defined later); however, tangent vectors and gradients have no definitions in metric spaces. In the following, we will characterize the gradient flow in Eq. ($\ref{eq:euclidean-gf}$) as the limit curve of a discrete-time scheme. We begin by discretizing the Ordinary Differential Equation (ODE) using the implicit Euler method,</p> \[\begin{align} \frac{x_{n+1} - x_{n}}{\tau} &amp;= -\nabla E(x_{n+1})\tag{2}\label{eq:back-euler}, \end{align}\] <p>which can equivalently be written as</p> \[\begin{align} \nabla\left(\frac{|x - x_{n}|^2}{2\tau} + E(x)\right)\Bigg|_{x=x_{n+1}} = 0\tag{3}\label{eq:back-euler-optim}. \end{align}\] <p>Eq. ($\ref{eq:back-euler-optim}$) looks like an optimality condition and consequently $x_{n+1}$ can be written as the solution to a minimization problem,</p> \[\begin{align} x_{n+1} = \mathrm{arg}\min\left(\frac{|x - x_{n}|^2}{2\tau} + E(x)\right)\tag{4}\label{eq:mms-gf}. \end{align}\] <p>We have converted the discrete scheme into a form that does not involve gradients. This discrete scheme can be generalized to a metric space $(\mathcal{X}, d)$ where the space $\mathcal{X}$ is endowed with the metric $d$. Let $F: \mathcal{X} \to \mathbb{R}$ be a functional that is lower semi continuous and bounded below<span style="font-size: 70%; color: gray;"><sup>1</sup></span>. The equivalent discrete scheme in this metric space is given by,</p> \[\begin{align} x_{n+1}^\tau \in \mathrm{arg}\min\left(\frac{d(x, x_{n}^\tau)^2}{2\tau} + F(x)\right)\tag{5}\label{eq:gmm-gf}, \end{align}\] <p>where we have replaced the Euclidean metric with the metric $d$. We can define a piecewise constant, continuous-time interpolation from the discrete scheme in Eq. ($\ref{eq:gmm-gf}$) as follows</p> \[x^{\tau}(t) = x_n^\tau\qquad \text{if}\:\: t \in ((n-1)\tau, n\tau]\tag{6}\label{eq:pc-interpolation}.\] <p>Gradient flows can now be characterized by studying the discrete scheme in the limit $\tau \to 0$. This discrete scheme is called the minimizing movement scheme and a curve $x: [0, T] \to \mathcal{X}$ is called Generalized Minimizing Movements (GMM) if there exists a sequence of time steps $\tau_j \to 0$ such that the sequence of piecewise constant curves in Eq. (\ref{eq:pc-interpolation}) converges uniformly to $x$.</p> <p>Before discussing the 2-Wasserstein space, we will define some quantities in metric spaces that will be referred to later.</p> <p><strong>Metric Derivative</strong> For a curve $x: [0, T] \to \mathcal{X}$ valued in a metric space $\mathcal{X}$, we can define the modulus of $x’(t)$ (i.e., the speed of the curve instead of its velocity as one would do in a vector space) as</p> \[|x'|(t) \triangleq \lim_{h \to 0} \frac{d(x(t), x(t + h))}{|h|}.\] <p><strong>AC Curves in a Metric Space</strong> A curve $x: [0, T] \to \mathcal{X}$ is said to be absolutely continuous (AC) if there exists a $g \in L^1([0, 1])$ such that $d(x(t_0), x(t_1)) \leq \int_{t_0}^{t_1}g(s)ds$ for every $t_0 &lt; t_1$.</p> <h2 id="background-in-optimal-transport">Background in Optimal Transport</h2> <p>Before defining the Wasserstein space of probability measures we introduce some ideas from optimal transport that will be used later. The optimal transport (OT) problem seeks to transport mass from a source measure $\mu$ to a target measure $\nu$ with respect to a given cost function $c$ in an “optimal” fashion. There are two popular formulations of the optimal transport problem due to Monge and Kantorovich.</p> <p><strong>Monge’s Formulation</strong> Monge’s formulation seeks to find an (optimal) transport map $T$ that pushes forward $\mu$ to $\nu$ (often denoted as $T_{\sharp}\mu = \nu$)<span style="font-size: 70%; color: gray;"><sup>2</sup></span> while minimizing the following quantity</p> \[\inf_{T_{\sharp}\mu = \nu} \int_{\mathcal{X}} c(T(x), x)d\mu(x)\tag{MP}.\] <p>Note that we’re looking for a transport map and are not allowed to “break up mass”. Consider the problem of transporting a dirac measure at $0$ to a measure that assigns equal mass to $-1$ and $+1$. It is not possible to construct a Monge map for such a transport problem.</p> <p><strong>Kantorovich’s Formulation</strong> Kanotrovich’s formulation seeks an (optimal) transport plan $\gamma$ that transports mass from $\mu$ to $\nu$ with respect to the cost function $c$ such that the marginals of $\gamma$ are $\mu$ and $\nu$.</p> \[\inf_{\gamma \in \Gamma(\mu, \nu)} \int_{\mathcal{X} \times \mathcal{Y}} c(x, y)d\gamma(x,y)\tag{KP}\label{eq:ot-kp}\] <p>where $\Gamma(\mu, \nu)$ is the set of transport plans with the correct marginals, i.e.,</p> \[\Gamma(\mu, \nu) = \{\gamma | (\pi_{\mathcal{X}})_\sharp\gamma = \mu, (\pi_{\mathcal{Y}})_\sharp\gamma=\nu\}\] <p>where \((\pi_{\mathcal{X}})_\sharp\gamma\) denotes the projection of $\gamma$ onto \(\mathcal{X}\). Monge’s formulation is a special case of Kanotrovich’s formulation such that whenever the optimal Monge map \(T^*\) exists, the corresponding Kantorovich transport plan is given by \(\gamma^* = (\mathrm{id} \times T^*)_\sharp\mu\) where \(\mathrm{id}\) is the identity map.</p> <p><strong>Kantorovich duality</strong> Kanotrovich’s formulation has an equivalent dual form given by</p> \[\sup_{\substack{f,g\\f(x)+g(y) \leq c(x, y)}} \int_\mathcal{X} fd\mu(x) + \int_\mathcal{Y} gd\nu(y)\tag{KPDual}\label{eq:ot-kpdual},\] <p>where the supremum runs over the set of bounded and continuous functions $f: \mathcal{X} \to \mathbb{R}$ and $g: \mathcal{Y} \to \mathbb{R}$ such that \(f(x)+g(y) \leq c(x, y)\). Eq. (\ref{eq:ot-kpdual}) can be written as a supremum over a single function $\psi$ with its corresponding $c$-transform<span style="font-size: 70%; color: gray;"><sup>3</sup></span> $\psi^c$,</p> \[\sup_{\psi \in \Psi^c} \int_\mathcal{X} \psi d\mu(x) + \int_\mathcal{Y} \psi^c d\nu(y)\tag{7}\label{eq:ot-kpdual-ctransform},\] <p>where \(\psi^c(y) = \inf_{x} \left\{c(x, y) - \psi(x)\right\}\) and $\Psi^c$ is the set of $c$-concave functions where a $c$-concave function is a function $\psi$ for which there exists a function $\varphi$ such that $\psi = \varphi^c$. The function(s) $\psi$ realizing the maximum in Eq. (\ref{eq:ot-kpdual-ctransform}) are called Kantorovich potentials.</p> <p><strong>OT with Quadratic Cost</strong> For the special case of optimal transport with the quadratic cost \(c(x, y) = \frac{\|x - y\|^2}{2}\), there exists a unique optimal transport plan of the form $(\mathrm{id} \times T^*)_\sharp\mu$ provided that $\mu$ is absolutely continuous. Further, there exists at least one Kantorovich potential $\psi$ such that its gradient $\nabla \psi$ is uniquely determined. The form of the transport plan implies the existence of an optimal Monge map \(T^*\) which is related to the potential $\psi$ by \(T^*(x) = x - \nabla\psi(x)\).</p> <h2 id="the-2-wasserstein-space">The 2-Wasserstein Space</h2> <p>Let $\mathcal{P}_2(\Omega)$ be the set of probability measures on a domain $\Omega \subset \mathbb{R}^d$ with finite second moments, i.e.,</p> \[\mathcal{P}_2(\Omega) \triangleq \left\{\mu \Big| \int_{\Omega}|x|^2d\mu(x) &lt; \infty\right\}.\] <p>The 2-Wasserstein distance between probability measures $\mu \in \mathcal{P}_2(\Omega)$ and $\nu \in \mathcal{P}_2(\Omega)$ is defined as,</p> \[{\color{blue}\mathcal{W}_2(\mu, \nu) \triangleq \left(\inf_{\gamma \in \Gamma(\mu, \nu)} \int \|x-y\|^2d\gamma(x,y)\right)^{1/2}}, \tag{W2}\label{eq:w2primal}\] <p>where $\Gamma(\mu, \nu)$ is a set of all possible couplings with marginals $\mu$ and $\nu$. We can see that $\mathcal{W}_2$ is equal to the square root of Eq. (\ref{eq:ot-kp}) with \(c(x,y) = \|x-y\|^2\). It can be shown that $\mathcal{W}_2$ satisfies the axioms of a metric on $\mathcal{P}_2(\Omega)$ and convergence with respect to $\mathcal{W}_2$ is equivalent to weak convergence of probability measures, i.e., for a sequence of measures \((\mu_k)_{k \in \mathbb{N}}\) in $\mathcal{P}_2(\Omega)$, \(\mu_k \to \mu \Longleftrightarrow \mathcal{W}_2(\mu_k, \mu) \to 0\). The 2-Wasserstein space $(\mathcal{P}_2(\Omega), \mathcal{W}_2)$ is the metric space of probability measures $\mathcal{P}_2(\Omega)$ endowed with the 2-Wasserstein ($\mathcal{W}_2$) metric.</p> <p><strong>AC Curves in the 2-Wasserstein Space</strong> Let \(\{\mu_t\}_{t\in[0,1]}\) be an absolutely continuous curve in $(\mathcal{P}_2(\Omega), \mathcal{W}_2)$. Then, for $t \in [0, 1]$ there exists a velocity field $\mathbf{v}_t \in L^2(\mu_t; \mathbb{R}^d)$ such that the continuity equation</p> \[{\color{blue}\frac{\partial \mu_t}{\partial t} + \nabla\cdot(\mathbf{v}_t\mu_t) = 0}\tag{CE}\label{eq:continuity-eq}\] <p>is satisfied and</p> \[\|\mathbf{v}_t\|_{L^2(\mu_t)} = |\mu'|(t).\tag{8}\label{eq:velocity-norm-metric-derivative}\] <p>The proof of these statements is beyond the scope of this article (frankly, I don’t know how to prove it). However, for the second statement, we can consider two measures $\mu_t$ and $\mu_{t+h}$. There are several ways to transport mass from $\mu_t$ to $\mu_{t+h}$ one of which is optimal in the OT sense. Let $T^*$ be the OT map between $\mu_t$ and $\mu_{t+h}$. We can define \(\mathbf{v}_t(x)\) as the discrete velocity of the particle $x$ at time $t$ given by \(\mathbf{v}_t(x) = (T^*(x)-x)/h\) (i.e., displacement/time). We can intuitively see that in the limit \(h \to 0\), Eq. (\ref{eq:velocity-norm-metric-derivative}) holds, since</p> \[\begin{align} \|\mathbf{v}_t\|_{L^2(\mu_t)} &amp;= \left(\int_{\mathbb{R}^d}\left|\frac{(T^*(x)-x)}{h}\right|^2d\mu_t(x)\right)^{1/2},\\ &amp;= \frac{1}{h}\mathcal{W}_2(\mu_{t},\mu_{t+h}). \end{align}\] <h2 id="gradient-flows-in-the-2-wasserstein-space">Gradient Flows in the 2-Wasserstein Space</h2> <p>Now that we have established that absolutely continuous curves in the 2-Wasserstein space satisfy the continuity equation, our task is to link Partial Differential Equations (PDEs) of the form of Eq. (\ref{eq:continuity-eq}) to the discrete-time minimizing movement scheme,</p> \[\begin{align} \rho_{n+1}^\tau &amp;\in \mathrm{arg}\min\left(\frac{\mathcal{W}_2(\rho, \rho_{n}^\tau)^2}{2\tau} + \mathcal{F}(\rho)\right)\tag{9}\label{eq:mms-w2},\\ \rho^\tau(t) &amp;= \rho^\tau_n\qquad \text{if}\:\: t \in ((n-1)\tau, n\tau], \end{align}\] <p>where $\mathcal{F}: \mathcal{P}_2(\Omega) \to \mathbb{R}$ is a functional on the 2-Wasserstein space that is lower semi-continuous and bounded below. Note that we now denote probability measures using $\rho$ to indicate the fact that they are absolutely continuous measures with smooth densities. Concretely, now our task is to find the velocity field $\mathbf{v}_t$ such that the solution to the continuity equation agrees with \(\lim_{\tau\to 0}\rho^\tau(t)\).</p> <p>Let us now investigate the optimality condition of Eq. (\ref{eq:mms-w2}). By analogy to the optimality condition for functions where we set the first derivative of the function equal to 0, we can set the first variation of a functional defined on the 2-Wasserstein space equal to a constant (Why?<span style="font-size: 70%; color: gray;"><sup>2</sup></span>). The first variation \({\color{purple}\frac{\delta\mathcal{G}}{\delta\rho}}\) of a functional \(\mathcal{G}: \mathcal{P}_2(\Omega) \to \mathbb{R}\) at a point \(\rho\), if it exists, is defined (up to additive constants) as</p> \[\frac{d}{d\epsilon} \mathcal{G}(\rho + \epsilon\chi)\Bigg|_{\epsilon = 0} = \int {\color{purple}\frac{\delta\mathcal{G}}{\delta\rho}(\rho)}d\chi(x).\] <p>Note that $\chi$ is chosen such that $\rho + \epsilon\chi \in \mathcal{P}_2(\Omega)$; this can be done by setting $\chi = \sigma - \rho$ for some $\sigma \in \mathcal{P}_2(\Omega)$.</p> <p>We now compute the first variation of the functional on the r.h.s. of Eq. (\ref{eq:mms-w2}) and set it equal to a constant,</p> \[\begin{align} \frac{\delta}{\delta \rho}\left[\frac{\mathcal{W}_2(\rho, \rho_{n}^\tau)^2}{2\tau} + \mathcal{F}(\rho)\right]\Bigg|_{\rho=\rho^\tau_{n+1}} &amp;= \mathrm{constant},\\ \left[\frac{1}{2\tau}\frac{\delta\mathcal{W}_2(\rho, \rho_{n}^\tau)^2}{\delta \rho} + \frac{\delta\mathcal{F}}{\delta \rho}(\rho)\right]\Bigg|_{\rho=\rho^\tau_{n+1}} &amp;= \mathrm{constant}.\tag{10}\label{eq:optimality-eq}\\ \end{align}\] <p>The first variation of $\mathcal{F}$ depends on its specific form; therefore, we begin by deriving an expression for the first variation of the squared 2-Wasserstein distance.</p> <p><strong>First Variation of the Squared 2-Wasserstein Distance</strong> We begin by writing the expression of the squared Wasserstein distance in its primal form,</p> \[\mathcal{W}^2_2(\mu, \nu) = 2\color{maroon}{\inf_{\gamma \in \Gamma(\mu, \nu)} \int \frac{\|x-y\|^2}{2}d\gamma(x,y)}.\] <p>The expression in <span style="color:#85144b">maroon</span> is the Kantorovich formulation of OT with the quadratic cost \(c(x,y) = \frac{\|x-y\|^2}{2}\). We have multiplied and divided the expression by 2 to utilize the properties of OT with the quadratic cost. Let us convert the expression into its dual form using $c$-concave functions,</p> \[\mathcal{W}^2_2(\mu, \nu) = 2{\sup_{\psi \in \Psi^c} \int \psi d\mu(x) + \int \psi^c d\nu(y)}.\] <p>Let $\psi_*$ be a Kantorovich potential that achieves the supremum in the above equation. We can now replace \(\psi\) with \(\psi_*\) and remove the \(\sup\) operator,</p> \[\mathcal{W}^2_2(\mu, \nu) = 2 \int \psi_* d\mu(x) + \int \psi_*^c d\nu(y).\] <p>Perturbing $\mu$ along $\chi$ and differentiating the resulting expression with respect to $\epsilon$ at $\epsilon=0$ we get,</p> \[\frac{d}{d\epsilon} \mathcal{W}^2_2(\mu + \epsilon\chi, \nu)\Bigg|_{\epsilon = 0} = \int {\color{purple}\underset{\frac{\delta\mathcal{W}^2_2(\mu, \nu)}{\delta\mu}}{\underbrace{2\psi_*}}} d\chi(x).\] <p><strong>Deriving an Expression for Particle Velocity</strong> The above equation shows that the first variation of the squared Wasserstein distance between measures $\mu$ and $\nu$ is equal to \(2\psi_*\) where \(\psi_*\) is the Kantorovich potential associated with optimal transport from $\mu$ to $\nu$ with the quadratic cost. We can substitute this result into Eq. (\ref{eq:optimality-eq}) and get,</p> \[\left[\frac{\varphi_*}{\tau} + \frac{\delta\mathcal{F}}{\delta \rho}(\rho^\tau_{n+1})\right] = \mathrm{constant}\tag{11}\label{eq:optimality-cond-2},\] <p>where $\varphi_*$ is the Kantorovich potential associated with optimal transport from $\rho^\tau_{n+1}$ to $\rho^\tau_{n}$. As mentioned earlier, the Kantorovich potential and the Monge transport map for the quadratic cost are related by \(T^*(x) = x - \nabla\varphi_*(x)\). Taking the gradient with respect to $x$ on both sides of Eq. (\ref{eq:optimality-cond-2}) and substituting \(\nabla\varphi_*(x) = -(T^*(x)-x)\) we get</p> \[\frac{(T^*(x)-x)}{\tau} = \nabla\frac{\delta\mathcal{F}}{\delta \rho}(\rho^\tau_{n+1})(x).\] <p>Note that the expression \(\frac{(T^*(x)-x)}{\tau}\) can be thought of as the discrete velocity of a particle moving backwards in time from $\rho^\tau_{n+1}$ to $\rho^\tau_{n}$ in an optimal transport sense. We can intuitively see how this expression would become the instantaneous velocity in the limit $\tau \to 0$. This gives us an expression for the instantaneous forward velocity of a particle,</p> \[{\color{blue}\mathbf{v}_t(x) = -\nabla\frac{\delta\mathcal{F}}{\delta \rho}(\rho_t)(x)}.\] <p>Plugging this expression into the continuity equation we get the expression of the gradient flow of a functional $\mathcal{F}$ in the Wasserstein space</p> \[{\color{blue}\frac{\partial \rho_t}{\partial t} - \nabla\cdot\left(\nabla\frac{\delta\mathcal{F}}{\delta \rho}(\rho_t)\rho_t\right) = 0}\] <h3 id="some-famous-examples-of-gradient-flows-in-mathcalp_2omega-mathcalw_2">Some famous examples of gradient flows in $(\mathcal{P}_2(\Omega), \mathcal{W}_2)$</h3> <p><strong>Negative Differential Entropy</strong> When the functional $\mathcal{F}$ is defined to be the negative differential entropy, i.e.,</p> \[\mathcal{F} \triangleq \int \rho(x)\log\rho(x)dx,\] <p>the velocity is given by \(\mathbf{v}_t = -\frac{1}{\rho(x)}\nabla\rho(x)\) and we recover the famous heat equation as the gradient flow of negative entropy in the 2-Wasserstein space,</p> \[{\color{blue}\frac{\partial \rho_t}{\partial t} - \Delta\rho = 0}.\tag{HE}\] <p><strong>$f$-Divergence</strong> When $\mathcal{F}$ is defined to be the $f$-divergence between $\rho_t$ and a fixed target density $\mu$, i.e.,</p> \[\mathcal{F} \triangleq \int f\left(\frac{\rho(x)}{\mu(x)}\right)\mu(x)dx,\] <p>where $f$ is a twice-differentiable convex function with $f(1) = 0$, the gradient flow is given by the following PDE,</p> \[\frac{\partial \rho_t}{\partial t} - \nabla\cdot\left(\rho(x)\nabla f'\left(\frac{\rho(x)}{\mu(x)}\right)\right) = 0.\] <p>For the special case of the KL divergence, i.e., $f = r\log r$, we recover the Fokker-Plank Equation,</p> \[{\color{blue}\frac{\partial \rho_t}{\partial t} + \nabla\cdot(\rho(x)\nabla\log\mu(x)) - \Delta\rho(x) = 0}.\tag{FPE}\] <h2 id="concluding-remarks">Concluding Remarks</h2> <p>Gradient flows have been a popular tool in the analysis of PDEs. Recently, gradient flows of various distances/divergences used in machine learning have been proposed [3, 4, 5, 6, 7] and have been used for generative modeling [4, 5, 7] and sample refinement in generative models [8], among other cool applications [5]. This article is my attempt at providing some background for readers unfamiliar with gradient flows.</p> <h2 id="references">References</h2> <div style="font-size: 80%"> <p>[1] Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview. Bulletin of Mathematical Sciences, 7(1), 2017.</p> <p>[2] Brittany Hamfeldt. Optimal Transport - Gradient Flows in the Wasserstein Metric. <a href="https://www.youtube.com/watch?v=zzGBxAqJV0Q">Video Lecture</a>, 2019.</p> <p>[3] Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum mean discrepancy gradient flow. In NeurIPS, 2019.</p> <p>[4] Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert Stoter. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In ICML, 2019.</p> <p>[5] Youssef Mroueh, Tom Sercu, and Anant Raj. Sobolev descent. In AISTATS, 2019.</p> <p>[6] Qiang Liu. Stein variational gradient descent as gradient flow. In NeurIPS, 2017.</p> <p>[7] Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang, Can Yang, and Shunkang Zhang. Deep generative learning via variational gradient flow. In ICML, 2019.</p> <p>[8] Abdul Fatir Ansari, Ming Liang Ang, and Harold Soh. Refining Deep Generative Models via Wasserstein Gradient Flows. arXiv preprint arXiv:2012.00780, 2020.</p> </div> <hr/> <h3 id="footnotes">Footnotes</h3> <p><span style="font-size: 70%; color: gray;"><sup>1</sup> Note that we no longer require $F$ to be convex but other regularity conditions (lower semi-continuity and boundedness from below) are required for the existence of minimizers.</span></p> <p><span style="font-size: 70%; color: gray;"><sup>2</sup> $T_{\sharp}\mu = \nu$ means that if we apply the map $T$ to samples from $\mu$ we get samples that are distributed according to $\nu$.</span></p> <p><span style="font-size: 70%; color: gray;"><sup>3</sup> Please check <a href="http://abdulfatir.com/Wasserstein-Distance/">this article</a> for an intuitive introduction to the $c$-transform of a function. </span></p> <p><span style="font-size: 70%; color: gray;"><sup>4</sup> The first variation is defined up to additive constants since $\chi$ is a zero-mean measure (it’s a difference of two probability measures).</span></p>]]></content><author><name></name></author><category term="optimal-transport"/><category term="gradient-flows"/><summary type="html"><![CDATA[Gradient flows have been a popular tool in the analysis of PDEs. Recently, various gradient flows have been studied in machine learning literature. This article is an introduction to the concept of gradient flows in the 2-Wasserstein space.]]></summary></entry><entry><title type="html">Monte Carlo Sampling using Langevin Dynamics</title><link href="https://abdulfatir.com//blog/2020/Langevin-Monte-Carlo/" rel="alternate" type="text/html" title="Monte Carlo Sampling using Langevin Dynamics"/><published>2020-04-17T21:01:00+00:00</published><updated>2020-04-17T21:01:00+00:00</updated><id>https://abdulfatir.com//blog/2020/Langevin-Monte-Carlo</id><content type="html" xml:base="https://abdulfatir.com//blog/2020/Langevin-Monte-Carlo/"><![CDATA[<p>Langevin Monte Carlo is a class of Markov Chain Monte Carlo (MCMC) algorithms that generate samples from a probability distribution of interest (denoted by $\pi$) by simulating the Langevin Equation. The Langevin Equation is given by</p> \[\lambda\frac{dX_t}{dt} = -\frac{\partial V(x)}{\partial x} + \eta(t), \tag{1}\label{eq:langevin}\] <p>where $X_t$ is the position of a particle in a potential $V(x)$ and $\eta(t)$ is a noise term. The dynamics in Eq. ($\ref{eq:langevin}$) is also commonly written as the following Stochastic Differential Equation (SDE)</p> \[\mathrm{d}X_t = \underset{\text{drift term}}{\underbrace{-\nabla V(x)\mathrm{d}t}} + \underset{\text{diffusion term}}{\underbrace{\sqrt{2}\mathrm{d}B_t}} \tag{2}\label{eq:itodiff}\] <p>which represents an Itô diffusion, where $\mathrm{d}B_t$ denotes the time derivative of standard Brownian motion.</p> <p>It can be shown that the SDE in Eq. ($\ref{eq:itodiff}$) has a unique invariant measure (or simply, a steady-state distribution) that does not change along the trajectory ($X_t$) of the particle. This means that if $X_0$ is distributed according to some probability density function $p_\infty$, then $X_t$ is also distributed according to $p_\infty$ for all $t \geq 0$. If we set the potential $V$ in Eq. ($\ref{eq:itodiff}$) cleverly such that $p_\infty = \pi$, then we can simulate the SDE (Eq. $\ref{eq:itodiff}$) to generate samples from $\pi$.</p> <center> <figure> <img style="display: box; margin: auto; width: 60%; height: 60%;" src="/assets/img/blogs/lmc/mala.png" alt="Langevin Dynamics"/> <figcaption align="center"> <b>Figure 1. A visualization of sampling using Langevin Dynamics.</b> </figcaption> </figure> </center> <h3 id="the-steady-state-distribution-choosing-the-potential">The steady-state distribution: choosing the potential</h3> <p>The Fokker-Plank equation is a partial differential equation (PDE) that describes the evolution of a probability distribution over time under the effect of drift forces and random (or noise) forces. The equivalent Fokker-Plank equation for the SDE in Eq. ($\ref{eq:itodiff}$) is given by</p> \[\frac{\partial p(x,t)}{\partial t} = \frac{\partial}{\partial x}\left[\frac{\partial V(x)}{\partial x}p(x,t)\right] + \frac{\partial^2p(x,t)}{\partial x^2}. \tag{3}\label{eq:fpe}\] <p>The steady-state solution of the Fokker-Plank equation is given by $\frac{\partial p(x,t)}{\partial t} = 0$. If $p_\infty$ is the steady-state distribution, we have</p> \[\frac{\partial p(x,t)}{\partial t} = \frac{\partial}{\partial x}\left[\frac{\partial V(x)}{\partial x}p_\infty(x) + \frac{\partial p_\infty(x)}{\partial x}\right] = \frac{\partial}{\partial x}J(x) = 0, \tag{4}\label{eq:steadystate}\] <p>where $J(x)$ denotes the probability “flux”. Eq. ($\ref{eq:steadystate}$) implies that $J(x)$ must be a constant; however, $p_\infty(x)$ and $\frac{\partial p_\infty(x)}{\partial x}$ must also satisfy certain boundary conditions. Specifically, the boundary condition that $J(x) = 0$ at infinity must be satisfied. Since $J(x) = 0$ at infinity and $J(x)$ is a constant, it must be equal to 0 everywhere. This leaves us with</p> \[J(x) = \frac{\partial V(x)}{\partial x}p_\infty(x) + \frac{\partial p_\infty(x)}{\partial x} = 0, \tag{5}\label{eq:zeroflux}\] <p>which has the solution</p> \[p_\infty(x) \propto \exp(-V(x)). \tag{6}\label{eq:gibbs}\] <p>Eq. ($\ref{eq:gibbs}$) represents a Gibbs distribution. This means that we can sample from energy-based models (EBMs) of the form $\pi(x) = \frac{\exp[-E(x)]}{Z}$, by setting $V(x) = E(x)$ in Eq. ($\ref{eq:itodiff}$). We can also write the distribution $\pi(x)$ as $\exp[\log\pi(x)]$, which means that we can set $V(x) = -\log\pi(x)$. It must be noted that we do not really need to know the normalization constant $Z$ for this to work because Eq. ($\ref{eq:itodiff}$) requires $\nabla\log\pi(x)$ and $\nabla Z = 0$ since $Z$ is a constant.</p> <h3 id="simulating-the-sde">Simulating the SDE</h3> <p>Having derived the form of the potential $V(x)$, we are now interested in simulating the following SDE to sample from its steady state distribution, i.e., $\pi(x)$,</p> \[\mathrm{d}X_t = \nabla \log\pi(x)\mathrm{d}t + \sqrt{2}\mathrm{d}B_t. \tag{7}\label{eq:finalsde}\] <p>The SDE can be discretized using a numerical method such as the Euler-Maruyama method. The Euler-Maruyama approximation of Eq. ($\ref{eq:finalsde}$) can be written as</p> \[X_{t + \tau} - X_{t} = \tau\nabla \log\pi(x) + \sqrt{2}(B_{t+\tau}-B_{t}), \tag{8}\label{eq:eulerapprox1}\] <p>where $\tau$ is the step-size and $(B_{t+\tau}-B_{t}) \sim \mathcal{N}(0,\tau)$. This allows us to write Eq. ($\ref{eq:eulerapprox1}$) as</p> \[X_{t + \tau} = X_{t} + \tau\nabla \log\pi(x) + \sqrt{2\tau}\xi, \tag{9}\label{eq:eulerapprox2}\] <p>where $\xi \sim \mathcal{N}(0,1)$. The time-step $\tau$ can also be changed over time.</p> <h4 id="unadjusted-langevin-algorithm">Unadjusted Langevin Algorithm</h4> <p>Eq. ($\ref{eq:eulerapprox2}$) gives us a method to sample from a probability distribution $\pi(x)$ by setting an initial seed $X_0$ and simulating the dynamics which, after a burn-in phase, will generate samples from $\pi(x)$. This algorithm is known as the Unadjusted Langevin Algorithm (ULA) which requires $\nabla \log\pi(x)$ to be $L$-Lipschitz for stability.</p> <h4 id="metropolis-adjusted-langevin-algorithm">Metropolis-adjusted Langevin Algorithm</h4> <p>The ULA always accepts the new sample proposed by Eq. ($\ref{eq:eulerapprox2}$). Metropolis-adjusted Langevin Algorithm (MALA), on the other hand, uses the Metropolis-Hastings algorithm to accept or reject the proposed sample. Since $\xi \sim \mathcal{N}(0,1)$, $X_{t + \tau} \sim \mathcal{N}(X_{t} + \tau\nabla \log\pi(x),2\tau)$ in Eq. ($\ref{eq:eulerapprox2}$). This means that the proposal distribution is given by</p> \[q(x'|x) \propto \exp\left(-\frac{\|x'-x-\tau\nabla \log\pi(x)\|^2}{2\cdot2\tau}\right).\] <p>The sample ($\tilde{X}_{k+1}$) proposed by Eq. ($\ref{eq:eulerapprox2}$) is accepted with the following acceptance probability</p> \[\alpha := \min\left(1, \frac{\pi(\tilde{X}_{k+1})q(X_k|\tilde{X}_{k+1})}{\pi(X_{k})q(\tilde{X}_{k+1}|X_k)}\right).\] <h3 id="visualizing-langevin-monte-carlo-sampling">Visualizing Langevin Monte Carlo Sampling</h3> <p>I set out to visualize these MCMC algorithms using <code class="language-plaintext highlighter-rouge">matplotlib.animation</code> to see how the distribution evolves over time. Unfortunately, writing to an MP4 file using <code class="language-plaintext highlighter-rouge">matplotlib.animation</code> is painfully slow and I could not find a simple way to speed it up. To solve this issue, I wrote a shell script to parallelize the generation of the chunks of the video and then combined them into one long video. The following video shows how samples are generated using MALA from a heart-shaped density given by</p> \[\pi(\mathbf{x}=\begin{bmatrix}x_1 &amp; x_2\end{bmatrix}^\top) \propto \exp\left(-\frac{0.8x_1^2 + \left(x_2-\sqrt[3]{x_1^2}\right)^2}{2^2}\right).\] <center> <iframe width="500" height="266" src="https://www.youtube.com/embed/cVn0kru3hL8?rel=0&amp;controls=0&amp;autohide=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""> </iframe> </center> <p>The code used to generate the video above can be found <a href="https://github.com/abdulfatir/langevin-monte-carlo">here</a>.</p> <h3 id="references">References</h3> <p>[1] Working with the Langevin and Fokker-Planck equations (<a href="https://www2.ph.ed.ac.uk/~dmarendu/ASP/Section16.pdf">notes</a>). <br/> [2] Chapter 4, Stochastic Processes and Applications, Grigorios A. Pavliotis (<a href="https://link.springer.com/book/10.1007/978-1-4939-1323-7">book</a>). <br/> [3] Wikipedia articles (<a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">a</a>, <a href="https://en.wikipedia.org/wiki/It%C3%B4_diffusion#Invariant_measures_for_gradient_flows">b</a>, <a href="https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">c</a>, <a href="https://en.wikipedia.org/wiki/Langevin_equation">d</a>).</p>]]></content><author><name></name></author><category term="sampling"/><summary type="html"><![CDATA[Langevin Monte Carlo is a class of Markov Chain Monte Carlo algorithms that generate samples from a probability distribution of interest by simulating the Langevin Equation. This post explores the basics of Langevin Monte Carlo.]]></summary></entry><entry><title type="html">1-Wasserstein distance: Kantorovich–Rubinstein duality</title><link href="https://abdulfatir.com//blog/2020/Wasserstein-Distance/" rel="alternate" type="text/html" title="1-Wasserstein distance: Kantorovich–Rubinstein duality"/><published>2020-04-08T21:01:00+00:00</published><updated>2020-04-08T21:01:00+00:00</updated><id>https://abdulfatir.com//blog/2020/Wasserstein-Distance</id><content type="html" xml:base="https://abdulfatir.com//blog/2020/Wasserstein-Distance/"><![CDATA[<p>The Kantorovich–Rubinstein distance, popularly known to the machine learning community as the Wasserstein distance, is a metric to compute the distance between two probability measures. The 1-Wasserstein is the most common variant of the Wasserstein distances (thanks to WGAN and its variants). Its dual form is generally used in an adversarial setup which is defined as</p> \[\sup_{f} \int fd\mu(x) - \int fd\nu(y) \quad\text{ where } f:\mathbb{R}^d\rightarrow\mathbb{R},\:\mathrm{Lip}(f) \leq 1 \tag{1}\label{eq:w1dual}\] <p>where $\mu$ and $\nu$ are probability measures, and $\mathrm{Lip}(f)$ denotes the Lipschitz constant of the function $f$. The function $f$ is realized using a neural network and the Lipschitz constraint is enforced using various techniques such as weight-clipping, gradient penalty, and spectral normalization. The neural network is trained to maximize the integral $\int fd\mu(x) - \int fd\nu(y)$.</p> <p>Eq. ($\ref{eq:w1dual}$) is the dual form of the Wasserstein distance which has the general form</p> \[W_p(\mu, \nu) = \left(\inf_{\gamma \in \Gamma(\mu, \nu)} \int d(x,y)^pd\gamma(x,y)\right)^{1/p} \tag{2}\label{eq:wpprimal}\] <p>where $d(x,y)$ is a distance metric between two points $x$ and $y$, and $\gamma$ is a coupling of the probability measures $\mu$ and $\nu$. Without going into too much jargon, a coupling $\gamma$ can be thought of as a joint distribution that belongs to the set of joint distributions $\Gamma(\mu, \nu)$ that have marginals $\mu$ and $\nu$. For the remainder of this note, we’ll set $d(x,y) = |x-y|$ and $p=1$.</p> <p>Generally in machine learning literature, the dual form and the Lipschitz constraints are directly introduced and there’s a lack of resources on the internet that address these basics. To this end, in this post I will explain how to arrive at the dual form (Eq. $\ref{eq:w1dual}$) from the primal form:</p> \[W_1(\mu, \nu) = \inf_{\gamma \in \Gamma(\mu, \nu)} \int \|x-y\|d\gamma(x,y). \tag{3}\label{eq:w1primal}\] <p>The treatment in this post will be a non-rigorous one. Please refer to [1] for a rigorous proof.</p> <center> <figure> <img style="display: box; margin: auto; width: 60%; height: 60%;" src="/assets/img/blogs/w1/mongemap.png" alt="Optimal Transport Map"/> <figcaption align="center"> <b>Figure 1. Optimal transport map $T$ from $\mu$ to $\nu$.</b> </figcaption> </figure> </center> <h2 id="kantorovich-duality">Kantorovich Duality</h2> <p>We first remove the $\gamma \in \Gamma(\mu, \nu)$ constraint from Eq. ($\ref{eq:w1primal}$) and add it as a supremum instead.</p> \[W_1(\mu, \nu) = \inf_{\gamma} \int \|x-y\|d\gamma(x,y) + \underset{\gamma \in \Gamma(\mu, \nu)\:\text{constraint}}{\underbrace{\sup_{f,g} \int fd\mu(x) + \int gd\nu(y) - \int (f(x)+g(y))d\gamma(x,y)}} \tag{4}\label{eq:derivstep1}\] <p>where $f$ and $g$ are absolutely integrable functions w.r.t. $\mu$ and $\nu$ respectively. Eq. ($\ref{eq:derivstep1}$) encodes the $\gamma \in \Gamma(\mu, \nu)$ constraint because the supremum is $0$ when $\gamma \in \Gamma(\mu, \nu)$ and is $\infty$ otherwise. This is because</p> \[\int (f(x)+g(y))d\gamma(x,y) = \int f(x)d\mu(x) + \int g(y)d\nu(y)\quad\text{if } \gamma \in \Gamma(\mu, \nu)\] <p>and would cancel the other terms. In any other case, $f$ and $g$ can be suitably chosen such that the supremum becomes $\infty$.</p> <p>We can now take the $\sup_{f,g}$ outside because the first term does not depend on $f$ and $g$,</p> \[W_1(\mu, \nu) = \inf_{\gamma}\sup_{f,g} \int (\|x-y\|-f(x)+g(y))d\gamma(x,y) + \int fd\mu(x) + \int gd\nu(y)\] <p>We can now invoke the minimax principle to replace the $\inf\sup$ with a $\sup\inf$ under certain conditions which are beyond the scope of this post.</p> \[W_1(\mu, \nu) = \sup_{f,g}\inf_{\gamma} \int (\|x-y\|-f(x)+g(y))d\gamma(x,y) + \int fd\mu(x) + \int gd\nu(y)\] <p>When $|x-y| \geq f(x)+g(y)$, the value of $\inf_{\gamma} \int (|x-y|-f(x)+g(y))d\gamma(x,y)$ is $0$ and is $-\infty$ otherwise. This can be added as a constraint in the equation as follows</p> \[W_1(\mu, \nu) = \sup_{\substack{f,g\\f(x)+g(y) \leq \|x-y\|}} \int fd\mu(x) + \int gd\nu(y) \tag{5}\label{eq:w1dualgeneral}\] <h3 id="c-transform">$c$-transform</h3> <p>How do we now find such $f$ and $g$ the maximize the r.h.s. of Eq. ($\ref{eq:w1dualgeneral}$)?</p> <p>Let’s assume that we have a function $f$ and we want to find the optimal $g$ corresponding to $f$ that achieves the supremum in Eq. ($\ref{eq:w1dualgeneral}$). We know that $\forall x,y \, f(x) + g(y) \leq |x-y|$. We can write this as follows</p> \[g(y) \leq \inf_{x} \left\{\|x-y\| - f(x)\right\}\] <p>because if $g(y) \leq |x-y| - f(x) \, \forall x,y$, then it must be true for the $x$ that minimizes the r.h.s. (which also makes sure that it is true for all other $x$s). Since, $g(y) \leq \inf_{x} |x-y| - f(x)$, the best we can do to maximize the r.h.s. in Eq. ($\ref{eq:w1dualgeneral}$) is set</p> <p>\(g(y) = \inf_{x} \left\{\|x-y\| - f(x)\right\} \tag{6}\label{eq:ctransform1}\).</p> <p>Eq. ($\ref{eq:ctransform1}$) gives us a function which is called the $c$-transform of $f$ and is often denoted by $f^c$,</p> \[f^c(y) = g(y) = \inf_{x} \left\{\|x-y\| - f(x)\right\}\] <p>It can be shown that $f^{cc} = f$. We can now write Eq. ($\ref{eq:w1dualgeneral}$) as</p> \[W_1(\mu, \nu) = \sup_{f} \int fd\mu(x) + \int f^cd\nu(y) \tag{7}\label{eq:w1dualffc}\] <h3 id="special-case-for-cost--x-y-">Special case for cost $| x-y |$</h3> <p>The above duality holds for any arbitrary cost $c(x,y)$, not just for $c(x,y)=|x-y|$. In the case of $|x-y|$, let’s derive the form of the dual problem (Eq. $\ref{eq:w1dualgeneral}$) when $f$ is 1-Lipschitz.</p> <p>When $f$ is 1-Lipschitz, $f^c$ is 1-Lipschitz too. This is true because for any given $x$,</p> \[f^c(y;x) = \|x-y\| - f(x)\] <p>is 1-Lipschitz and therefore the infimum of the r.h.s. $f^c(y) = \inf_{x} |x-y| - f(x)$ is 1-Lipschitz.</p> <p>Since $f^c$ is 1-Lipschitz, for all $x$ and $y$ we have</p> \[\begin{align} &amp;|f^c(y) - f^c(x)| \leq 1\cdot\|y-x\|\\ &amp;\implies -1\cdot\|x-y\| \leq f^c(y) - f^c(x) \leq 1\cdot\|x-y\|\\ &amp;\implies - f^c(x) \leq \|x-y\| - f^c(y)\tag{8}\label{ineq:lip} \end{align}\] <p>Since Eq. ($\ref{ineq:lip}$) is true for all $x$ and $y$,</p> \[\begin{align} &amp;\implies-f^c(x) \leq \inf_{y} \left\{\|x-y\| - f^c(y)\right\}\\ &amp;\implies-f^c(x) \leq \underset{f^{cc}(x)}{\underbrace{\inf_{y} \left\{\|x-y\| - f^c(y)\right\}}} \leq -f^c(x)\tag{9}\label{ineq:sandwich} \end{align}\] <p>where the right inequality follows by choosing $y = x$ in the infimum. We know that $f^{cc} = f$. This means that $-f^c(x)$ must be equal to $f(x)$ for Eq. ($\ref{ineq:sandwich}$) to hold.</p> <p>Substituting $f^c = -f$ in Eq. ($\ref{eq:w1dualffc}$), we get</p> \[\sup_{\substack{f\\\mathrm{Lip}(f) \leq 1}} \int fd\mu(x) - \int fd\nu(y)\] <p>which is the dual form of 1-Wasserstein distance.</p> <h2 id="references">References</h2> <p>[1] Cédric Villani: Topics in Optimal Transportation, Chapter 1. <br/> [2] Vincent Herrmann : Wasserstein GAN and the Kantorovich-Rubinstein Duality (<a href="https://vincentherrmann.github.io/blog/wasserstein/">Blog</a>). <br/> [3] Marco Cuturi: A Primer on Optimal Transport (<a href="https://www.youtube.com/watch?v=1ZiP_7kmIoc">Talk</a>).</p>]]></content><author><name></name></author><category term="probability-metrics"/><category term="optimal-transport"/><summary type="html"><![CDATA[The 1-Wasserstein distance is a popular integral probability metric. In this post, the dual form of the 1-Wasserstein distance is derived from its primal form.]]></summary></entry><entry><title type="html">Normalizing Flows: Planar and Radial Flows</title><link href="https://abdulfatir.com//blog/2018/Normalizing-Flows-Part-1/" rel="alternate" type="text/html" title="Normalizing Flows: Planar and Radial Flows"/><published>2018-09-28T21:01:00+00:00</published><updated>2018-09-28T21:01:00+00:00</updated><id>https://abdulfatir.com//blog/2018/Normalizing-Flows-Part-1</id><content type="html" xml:base="https://abdulfatir.com//blog/2018/Normalizing-Flows-Part-1/"><![CDATA[<p>Simple distributions (e.g., Gaussian) are often used as likelihood distributions. However, the true distribution is often far from this simple distribution and this results in issues such as blurry reconstructions in the case of images. Latent variable models such as VAEs often set the prior distribution $p(\mathbf{z})$ to a factorial multivariate Gaussian distribution. Such a simplistic assumption hampers the model in multiple ways. For instance, this does not allow a multi-modal latent space distribution. Normalizing Flows allow transformation of samples from a simple distribution (subsequently denoted by $q_0$) to samples from a complex distribution by applying a series of invertible functions.</p> <h3 id="distribution-of-a-simple-transformation-of-a-rv">Distribution of a Simple Transformation of a RV</h3> <p>Before jumping into normalizing flows, let’s consider a simple univariate distribution $p(x)=2x$ with support $x\in[0,1]$. Define a function $y = f(x) = x^2$. Note that $f(x)$ is monotonically increasing in $[0,1]$. What is the PDF of the random variable $y$?</p> <p>We can compute $p(y)$ using the CDFs as follows.</p> \[\begin{align} F_{Y}(y) &amp;= P(Y \leq y)\\ &amp;= P(X^2 \leq y)\\ &amp;= P(X \leq \sqrt{y})\\ &amp;= F_{X}(\sqrt{y})\\ \end{align}\] <p>Now, $p(y) = F_{Y}’(y) = \frac{dF_{X}(\sqrt{y})}{dy}$ where</p> \[\begin{align} F_{X}(\sqrt{y}) &amp;= \int_{0}^{\sqrt{y}} p(x) dx\\ &amp;=\left[2\frac{x^2}{2}\right]_{0}^{\sqrt{y}}\\ &amp;=y \end{align}\] <p>differentiating w.r.t. $y$ we get $\frac{d(y)}{dy} = 1$ which means that $p(y) = \mathcal{U}(0,1)$.</p> <h3 id="change-of-variables">Change of Variables</h3> <p>The method described above can be extended to multivariate distributions $q_0(\mathbf{z})$ and smooth invertible mappings $f: \mathbb{R}^d\Rightarrow\mathbb{R}^d$. Samples $\mathbf{z} \sim q_0(\mathbf{z})$ can be transformed using $f$ to give $\mathbf{y}=f(\mathbf{z})$. The PDF of $\mathbf{y}$ is given by</p> \[q_1(\mathbf{y}) = q_0(\mathbf{z})\left|\det \frac{\partial f^{-1}}{\partial \mathbf{y}}\right| = q_0(\mathbf{z})\left|\det \frac{\partial f}{\partial \mathbf{z}}\right|^{-1}\tag{1}\label{eq:cov}\] <p>where the second equality comes from the inverse-function theorem.</p> <p>Rezende and Mohamed (2015) proposed two different families of invertible transformations: planar flow and radial flow.</p> <h2 id="planar-flow">Planar Flow</h2> <p>Planar flows use functions of form</p> \[\begin{align} f(\mathbf{z}) = \mathbf{z} + \mathbf{u}h(\mathbf{w}^\top\mathbf{z} + b) \tag{2}\label{eq:planarfn} \end{align}\] <p>where $\mathbf{u},\mathbf{w}\in \mathbb{R}^d$, $b \in \mathbb{R}$, and $h$ is an element-wise non-linearity such as $\tanh$.</p> <p>The Jacobian is then given by</p> \[\begin{align*} \frac{\partial f(\mathbf{z})}{\partial \mathbf{z}} = \mathbf{I} + \mathbf{u}h'(\mathbf{w}^\top\mathbf{z} + b)\mathbf{w}^\top \end{align*}\] <p>Now, using the matrix determinant lemma</p> \[\begin{align*} \det\frac{\partial f(\mathbf{z})}{\partial \mathbf{z}} &amp;= (1 + h'(\mathbf{w}^\top\mathbf{z} + b)\mathbf{w}^\top\mathbf{I}^{-1}\mathbf{u})\det(\mathbf{I})\\ &amp;=(1 + h'(\mathbf{w}^\top\mathbf{z} + b)\mathbf{w}^\top\mathbf{u})\tag{3}\label{eq:planar-det} \end{align*}\] <h3 id="example">Example</h3> <p>Let’s look at a specific example for $\mathbf{z}\in\mathbb{R}^2$. We will apply a planar flow to $\mathbf{z}$ to get $\mathbf{y} = f(\mathbf{z})$.</p> \[\begin{align} q_0(\mathbf{z}) &amp;= \mathcal{N}(\mathbf{z};\mathbf{0},\mathbf{I})\\ \mathbf{w} &amp;= \begin{bmatrix}5 &amp; 0\end{bmatrix}^\top\\ \mathbf{u} &amp;= \begin{bmatrix}1 &amp; 0\end{bmatrix}^\top\\ b &amp;= 0\\ h(\mathbf{x}) &amp;= \tanh(\mathbf{x}) \end{align}\] <p>The determinant of the Jacobian can be computed using Eq. ($\ref{eq:planar-det}$) and the analytic PDF $q_1(\mathbf{y})$ can then be computed using Eq. ($\ref{eq:cov}$).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Function to compute q0(z)
</span><span class="k">def</span> <span class="nf">mvn_pdf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span> <span class="n">sig</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])):</span>
    <span class="kn">import</span> <span class="n">numpy.linalg</span> <span class="k">as</span> <span class="n">LA</span>
    <span class="n">sqrt_det_2pi_sig</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">LA</span><span class="p">.</span><span class="nf">det</span><span class="p">(</span><span class="n">sig</span><span class="p">))</span>
    <span class="n">sig_inv</span> <span class="o">=</span> <span class="n">LA</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">sig</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">sig_inv</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
                  <span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt_det_2pi_sig</span>
</code></pre></div></div> <p>Let’s set up the required functions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">5.</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">h_prime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="nf">h</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">u</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span>
<span class="k">def</span> <span class="nf">det_J</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">psi</span> <span class="o">=</span> <span class="nf">h_prime</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span>
    <span class="n">det</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">psi</span><span class="p">,</span> <span class="n">u</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">det</span>
</code></pre></div></div> <p>Let’s see how applying $f$ (Eq. $\ref{eq:planarfn}$) to points in a uniform grid moves them in the 2D space.</p> <center> <figure> <img style="display: box; margin: auto; width: 80%; height: 80%;" src="/assets/img/blogs/nf/planar-points.png" alt="Planar Flow Points"/> <figcaption align="center"> <b>Figure 1.</b> </figcaption> </figure> </center> <p>Let’s plot the analytic density $q_0(\mathbf{z})$ along with the empirical density by plotting a 2D histogram of samples from $q_0(\mathbf{z})$.</p> <center> <figure> <img style="display: box; margin: auto; width: 30%; height: 30%;" src="/assets/img/blogs/nf/planar-q0.png" alt="q0"/> <img style="display: box; margin: auto; width: 30%; height: 30%;" src="/assets/img/blogs/nf/planar-q0-emp.png" alt="q0 emp"/> <figcaption align="center"> <b>Figure 2.</b> Analytic and empirical densities $q_0(\mathbf{z})$ </figcaption> </figure> </center> <p>Now, let’s plot the analytic density $q_1(\mathbf{y})$ computed using Eq. (\ref{eq:cov}) along with the empirical density of $\mathbf{y}$.</p> <center> <figure> <img style="display: box; margin: auto; width: 30%; height: 30%;" src="/assets/img/blogs/nf/planar-q1.png" alt="q1"/> <img style="display: box; margin: auto; width: 30%; height: 30%;" src="/assets/img/blogs/nf/planar-q1-emp.png" alt="q1 emp"/> <figcaption align="center"> <b>Figure 3.</b> Analytic and empirical densities $q_1(\mathbf{y})$ </figcaption> </figure> </center> <p>The analytic and empirical densities look similar. We’ve transformed a unimodal $q_0(\mathbf{z})$ into a bimodal $q_1(\mathbf{y})$ by applying a one level planar flow. Such functions can be successively applied multiple times to obtain a far more complex distribution.</p> <h2 id="radial-flow">Radial Flow</h2> <p>Radial flows use functions of the form</p> \[\begin{align*} f(\mathbf{z}) = \mathbf{z} + \beta h(\alpha,r)(\mathbf{z}-\mathbf{z}_0) \tag{4}\label{eq:radialfn} \end{align*}\] <p>where $\alpha \in \mathbb{R}^+$, $\beta \in \mathbb{R}$, $h(\alpha,r) = (\alpha + r)^{-1}$ and $r = \vert\vert\mathbf{z} - \mathbf{z}_0\vert\vert$.</p> <p>The Jacobian is then given by</p> \[\begin{align*} \frac{\partial f(\mathbf{z})}{\partial \mathbf{z}} &amp;= \mathbf{I} + \beta\left((\mathbf{z}-\mathbf{z}_0)h'(\alpha,r)\frac{\partial r}{\partial \mathbf{z}} + h(\alpha,r)\mathbf{I}\right)\\ &amp;=(1+\beta h(\alpha,r))\mathbf{I} + \beta h'(\alpha,r)(\mathbf{z}-\mathbf{z}_0)\frac{(\mathbf{z}-\mathbf{z}_0)^\top}{||\mathbf{z}-\mathbf{z}_0||} \end{align*}\] <p>Let $\gamma = (1+\beta h(\alpha,r))$. Again, using the matrix determinant lemma</p> \[\begin{align*} \det\frac{\partial f(\mathbf{z})}{\partial \mathbf{z}} &amp;= \left(1 + \beta h'(\alpha,r)\frac{(\mathbf{z}-\mathbf{z}_0)^\top}{||\mathbf{z}-\mathbf{z}_0||}\frac{\mathbf{I}}{\gamma}(\mathbf{z}-\mathbf{z}_0)\right)\det(\gamma\mathbf{I})\\ &amp;=\left(\frac{1 + \beta h(\alpha,r) + \beta h'(\alpha,r)||\mathbf{z}-\mathbf{z}_0||}{(1+\beta h(\alpha,r))}\right)(1+\beta h(\alpha,r))^d\\ &amp;=\left(1 + \beta h(\alpha,r) + \beta h'(\alpha,r)r\right)(1+\beta h(\alpha,r))^{d-1}\tag{5}\label{eq:radial-det} \end{align*}\] <h3 id="example-1">Example</h3> <p>Let’s look at a specific example for $\mathbf{z}\in\mathbb{R}^2$. We will apply a radial flow to $\mathbf{z}$ to get $\mathbf{y} = f(\mathbf{z})$.</p> \[\begin{align} q_0(\mathbf{z}) &amp;= \mathcal{N}(\mathbf{z};\mathbf{0},\mathbf{I})\\ \mathbf{z}_0 &amp;= \begin{bmatrix}1 &amp; 0\end{bmatrix}^\top\\ \alpha &amp;= 2\\ \beta &amp;= 5 \end{align}\] <p>The determinant of the Jacobian can be computed using Eq. ($\ref{eq:radial-det}$) and the analytic PDF $q_1(\mathbf{y})$ can then be computed using Eq. ($\ref{eq:cov}$).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">r</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">r</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">h_prime</span><span class="p">(</span><span class="n">r</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">r</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">LA</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">z0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="nf">h</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">z0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
<span class="k">def</span> <span class="nf">det_J</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">n_dims</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">LA</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">z0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="nf">h</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    <span class="n">det</span> <span class="o">=</span> <span class="p">(</span><span class="n">tmp</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="nf">h_prime</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="o">*</span> <span class="n">r</span><span class="p">)</span> <span class="o">*</span> <span class="n">tmp</span> <span class="o">**</span> <span class="p">(</span><span class="n">n_dims</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">det</span>
</code></pre></div></div> <p>Let’s see how applying $f$ (Eq. $\ref{eq:radialfn}$) to points in a uniform grid moves them in the 2D space.</p> <center> <figure> <img style="display: box; margin: auto; width: 80%; height: 80%;" src="/assets/img/blogs/nf/radial-points.png" alt="Planar Flow Points"/> <figcaption align="center"> <b>Figure 4.</b> </figcaption> </figure> </center> <p>Let’s plot the analytic density $q_0(\mathbf{z})$ along with the empirical density by plotting a 2D histogram of samples from $q_0(\mathbf{z})$.</p> <center> <figure> <img style="display: box; margin: auto; width: 30%; height: 30%;" src="/assets/img/blogs/nf/radial-q0.png" alt="q0"/> <img style="display: box; margin: auto; width: 30%; height: 30%;" src="/assets/img/blogs/nf/radial-q0-emp.png" alt="q0 emp"/> <figcaption align="center"> <b>Figure 5.</b> Analytic and empirical densities $q_0(\mathbf{z})$ </figcaption> </figure> </center> <p>Finally, let’s plot the analytic density $q_1(\mathbf{y})$ computed using Eq. (\ref{eq:cov}) along with the empirical density of $\mathbf{y}$.</p> <center> <figure> <img style="display: box; margin: auto; width: 30%; height: 30%;" src="/assets/img/blogs/nf/radial-q1.png" alt="q1"/> <img style="display: box; margin: auto; width: 30%; height: 30%;" src="/assets/img/blogs/nf/radial-q1-emp.png" alt="q1 emp"/> <figcaption align="center"> <b>Figure 6.</b> Analytic and empirical densities $q_1(\mathbf{y})$ </figcaption> </figure> </center> <p>Not all functions of the forms given in Eqs. ($\ref{eq:planarfn}$) and ($\ref{eq:radialfn}$) are invertible. Some conditions need to be satisfied for them to be invertible. (See appendix in [1])</p> <p>The complete code used in this post can be found in <a href="https://github.com/abdulfatir/normalizing-flows">this repository</a>.</p> <h2 id="references">References</h2> <p>[1] Rezende, D.J. and Mohamed, S., 2015. Variational inference with normalizing flows. arXiv preprint <a href="https://arxiv.org/abs/1505.05770">arXiv:1505.05770</a>. <br/> [2] Blog posts <a href="http://akosiorek.github.io/ml/2018/04/03/norm_flows.html">1</a> and <a href="https://casmls.github.io/general/2016/09/25/normalizing-flows.html">2</a>.</p>]]></content><author><name></name></author><category term="generative-models"/><category term="normalizing-flows"/><summary type="html"><![CDATA[A normalizing flow is a great tool that can transform simple probability distributions into very complex ones by applying a series of invertible functions to samples from the simple distribution. This post explores two simple flows introduced by Rezende and Mohamed (2015) –– Planar Flow and Radial Flow.]]></summary></entry><entry><title type="html">Implicit Reparameterization Gradients</title><link href="https://abdulfatir.com//blog/2018/Implicit-Reparameterization/" rel="alternate" type="text/html" title="Implicit Reparameterization Gradients"/><published>2018-09-21T21:01:00+00:00</published><updated>2018-09-21T21:01:00+00:00</updated><id>https://abdulfatir.com//blog/2018/Implicit-Reparameterization</id><content type="html" xml:base="https://abdulfatir.com//blog/2018/Implicit-Reparameterization/"><![CDATA[<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script> <p>Backpropagation through a stochastic node is an important problem in deep learning. The optimization of \(\mathbb{E}_{q_\phi(\mathbf{z})}[f(\mathbf{z})]\) requires computation of \(\nabla_\phi\mathbb{E}_{q_\phi(\mathbf{z})}[f(\mathbf{z})]\). Stochastic variational inference requires the computation of the gradient of one such expectation.</p> <p>\(\definecolor{mBrown}{RGB}{188,99,16}\) \(\begin{align*} \mathcal{L}(\mathbf{x},\theta, \phi) = \color{mBrown}{\mathbb{E}_{q_{\phi}(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]} - \color{black}\mathrm{KL}(q_{\phi}(\mathbf{z}|\mathbf{x})||p(\mathbf{z})) \end{align*}\)</p> <p>Earlier methods of gradient computation include score-function-based estimators (REINFORCE) and pathwise gradient estimators (reparameterization trick). Recent works have proposed using reparametrizable surrograte distributions such as <strong>Gumbel-Softmax</strong> for <strong>Categorical</strong>, <strong>Kumaraswamy</strong> for <strong>Beta</strong>, etc. Other recent works such as Generalized Reparameterization Gradients (GRG) and Rejection Sampling Variational Inference (RSVI) have sought to build a generalized framework for gradient computation.</p> <h2 id="explicit-reparameterization">Explicit Reparameterization</h2> <p>It requires a standardization function \(\mathcal{S}_\phi(\mathbf{z})\) such that \(\mathcal{S}_\phi(\mathbf{z}) = \varepsilon \sim p(\varepsilon)\). It also requires \(\mathcal{S}_\phi(\mathbf{z})\) to be invertible. \(\mathbf{z}\sim q_\phi(\mathbf{z}) \Leftrightarrow \mathbf{z} = \mathcal{S}_\phi^{-1}(\varepsilon)\) and \(\varepsilon \sim p(\varepsilon)\).</p> \[\begin{align*} \nabla_\phi\mathbb{E}_{q_\phi(\mathbf{z})}[f(\mathbf{z})] &amp;= \mathbb{E}_{q(\varepsilon)}[\nabla_\phi f(\mathcal{S}_\phi^{-1}(\varepsilon))]\\ &amp;= \mathbb{E}_{q(\varepsilon)}[\nabla_\mathbf{z}f(\mathcal{S}_\phi^{-1}(\varepsilon))\nabla_\phi\mathcal{S}_\phi^{-1}(\varepsilon)] \end{align*}\] <h2 id="implicit-reparameterization">Implicit Reparameterization</h2> <p>Implicit Reparameterization eliminates the restrictive requirement of an invertible \(\mathcal{S}_\phi(\mathbf{z})\).</p> <center> <figure> <img style="display: box; margin: auto; width: 30%; height: 30%;" src="/assets/img/blogs/implicit/implicit.png" alt="Implicit Reparameterization"/> <figcaption align="center"> Figure 1. </figcaption> </figure> </center> \[\begin{align} \nabla_\phi\mathbb{E}_{q_\phi(\mathbf{z})}[f(\mathbf{z})] &amp;= \mathbb{E}_{q(\varepsilon)}[\nabla_\mathbf{z}f(\mathcal{S}_\phi^{-1}(\varepsilon))\nabla_\phi\mathcal{S}_\phi^{-1}(\varepsilon)]\\ &amp;=\mathbb{E}_{q_\phi(\mathbf{z})}[\nabla_\mathbf{z}f(\mathbf{z})\nabla_\phi\mathbf{z}] \end{align}\] \[\begin{align} \frac{d\mathcal{S}_\phi(\mathbf{z})}{d\phi} = \frac{d\varepsilon}{d\phi} &amp;= 0\tag{1}\\ \frac{\partial\mathcal{S}_\phi(\mathbf{z})}{\partial\mathbf{z}}\frac{d\mathbf{z}}{d\phi} + \frac{\partial\mathcal{S}_\phi(\mathbf{z})}{\partial\phi} &amp;= 0\tag{2} \end{align}\] \[\begin{align} \nabla_\phi\mathbf{z} = -(\nabla_\mathbf{z}\mathcal{S}_\phi(\mathbf{z}))^{-1}\nabla_\phi\mathcal{S}_\phi(\mathbf{z}) \end{align}\] <p>where Eq. (1) uses the fact that the total derivative of noise with respect to the distribution parameters is 0 and Eq. (2) applies the multivariate chain rule based on Figure 1.</p> <h2 id="examples">Examples</h2> <h3 id="normal-distribution">Normal Distribution</h3> <p>The standardization function for the normal distribution is \(\mathcal{S}_\phi(\mathbf{z}) = \frac{\mathbf{z}-\mu}{\sigma} \sim \mathcal{N}(\mathbf{0},\mathbf{I})\).</p> <ul> <li>Explicit Reparameterization: \(\mathcal{S}_\phi^{-1}(\varepsilon) = \mu + \sigma\varepsilon \Rightarrow \frac{d\mathbf{z}}{d\mu} = 1\) and \(\frac{d\mathbf{z}}{d\sigma} = \varepsilon\).</li> <li>Implicit Reparameterization: \(\frac{d\mathbf{z}}{d\mu} = -\frac{d\mathcal{S}_\phi(\mathbf{z})/d\mu}{d\mathcal{S}_\phi(\mathbf{z})/d\mathbf{z}} = 1\) and \(\frac{d\mathbf{z}}{d\sigma} = -\frac{d\mathcal{S}_\phi(\mathbf{z})/d\sigma}{d\mathcal{S}_\phi(\mathbf{z})/d\mathbf{z}} = \frac{\mathbf{z}-\mu}{\sigma}\).</li> </ul> <h3 id="using-cumulative-distribution-function">Using Cumulative Distribution Function</h3> <p>The CDF can be used as a standardization function by using the property that for a random variable \(\mathbf{z}\), the random variable \(\mathbf{y} = F_\phi(\mathbf{z})\) has the uniform distribution on \([0,1]\) where \(F_\phi\) is the CDF. The gradient can then be computed as follows.</p> \[\nabla_\phi\mathbf{z} = -\frac{\nabla_\phi F_\phi(\mathbf{z})}{q_\phi(\mathbf{z})}\] <h2 id="conclusion">Conclusion</h2> <p>Implicit Reparameterization allows stochastic backpropagation through a variety of distributions such as truncated, mixtures, gamma, Von-Mises, Beta, etc. Check out <a href="/assets/pdf/implicit2018.pdf">these slides</a> and <a href="https://arxiv.org/abs/1805.08498">the paper</a>.</p> <h3 id="references">References</h3> <p>[1] Figurnov, M., Mohamed, S. and Mnih, A., 2018. Implicit Reparameterization Gradients. arXiv preprint arXiv:1805.08498.<br/> [2] Jang, E., Gu, S. and Poole, B., 2016. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144.<br/> [3] Kingma, D.P. and Welling, M., 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.<br/> [4] Naesseth, C.A., Ruiz, F.J., Linderman, S.W. and Blei, D.M., 2016. Reparameterization gradients through acceptance-rejection sampling algorithms. arXiv preprint arXiv:1610.05683.<br/> [5] Ruiz, F.R., AUEB, M.T.R. and Blei, D., 2016. The generalized reparameterization gradient. In Advances in neural information processing systems (pp. 460-468).</p>]]></content><author><name></name></author><category term="generative-models"/><category term="variational-inference"/><summary type="html"><![CDATA[Backpropagation through a stochastic node is an important problem in deep learning. Implicit reparameterization gradients go beyond the reparameterization trick to address the problem of efficient gradient computation in such a setting.]]></summary></entry></feed>